{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e88267f9-7b72-4fc0-99c1-b4312bde5be3",
      "metadata": {
        "id": "e88267f9-7b72-4fc0-99c1-b4312bde5be3"
      },
      "source": [
        "# **BioImage Model Zoo Example notebook**\n",
        "\n",
        "---\n",
        "\n",
        "This notebook provides examples of how to load pretrained deep learning models from [BioImage Model Zoo](https://bioimage.io), use them to process new images, and finetune them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02b0904",
      "metadata": {
        "id": "a02b0904"
      },
      "source": [
        "## **1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c446cf",
      "metadata": {
        "id": "61c446cf"
      },
      "source": [
        "### **1.1. Install required dependencies**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7bfc0d0",
      "metadata": {
        "cellView": "form",
        "id": "b7bfc0d0"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install dependencies\n",
        "#@markdown #### DO NOT RESTART THE SESSION UNTIL THE CELL FINISHES RUNNING\n",
        "#@markdown #### This may take few minutes\n",
        "\n",
        "!pip install -q bioimageio.core==0.6.8\n",
        "!pip install -q matplotlib==3.9.0\n",
        "!pip install -q imageio==2.31.2\n",
        "!pip install -q numpy==1.23.5\n",
        "!pip install -q torch==2.2.0\n",
        "!pip install -q onnxruntime==1.18.0\n",
        "!pip install -q pooch==1.8\n",
        "!pip install -q marshmallow==3.21.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97a2705",
      "metadata": {
        "id": "e97a2705"
      },
      "source": [
        "### **1.2. Connect to your Google Drive to access training data**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6576a715",
      "metadata": {
        "cellView": "form",
        "id": "6576a715"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Run this cell to connect Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306d47df-e0d3-4856-a534-36aa032a2c83",
      "metadata": {
        "id": "306d47df-e0d3-4856-a534-36aa032a2c83",
        "tags": []
      },
      "source": [
        "### **1.3. Load BioImageIO dependencies**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b1ed61-e608-4403-8c1c-f0398bc31a26",
      "metadata": {
        "cellView": "form",
        "id": "f7b1ed61-e608-4403-8c1c-f0398bc31a26",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to load the dependencies and functions\n",
        "\n",
        "# If you'd rather read the warning messages, please comment the follwing two lines.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load BioImage Model Zoo packages\n",
        "from bioimageio.core.digest_spec import create_sample_for_model\n",
        "from bioimageio.core import predict, create_prediction_pipeline, load_description, test_model\n",
        "\n",
        "import bioimageio.spec\n",
        "from bioimageio.spec import save_bioimageio_package\n",
        "from bioimageio.spec.utils import load_array, download\n",
        "from bioimageio.spec.model.v0_5 import (ModelDescr, ArchitectureFromFileDescr, Author, CiteEntry,\n",
        "                                        Version, Doi, HttpUrl, LicenseId,\n",
        "                                        WeightsDescr, PytorchStateDictWeightsDescr, TorchscriptWeightsDescr,\n",
        "                                        InputTensorDescr, OutputTensorDescr, TensorId, LinkedDataset,\n",
        "                                        FileDescr, IntervalOrRatioDataDescr, Identifier, SizeReference,\n",
        "                                        BatchAxis, ChannelAxis, SpaceInputAxis, SpaceOutputAxis)\n",
        "\n",
        "\n",
        "# Load other packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pooch\n",
        "import json\n",
        "import os\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from imageio import imwrite as imsave\n",
        "from imageio import imread\n",
        "from pathlib import Path\n",
        "from ruyaml import YAML\n",
        "\n",
        "# URL pointing the file with the collection from the BIoimage Model Zoo\n",
        "COLLECTION_URL = \"https://raw.githubusercontent.com/bioimage-io/collection-bioimage-io/gh-pages/collection.json\"\n",
        "\n",
        "# Download the colection\n",
        "collection_path = Path(pooch.retrieve(COLLECTION_URL, known_hash=None))\n",
        "with collection_path.open() as f:\n",
        "    collection = json.load(f)\n",
        "\n",
        "# Get all the URLs of the models in the downloaded collection\n",
        "model_urls = [entry[\"rdf_source\"] for entry in collection[\"collection\"] if entry[\"type\"] == \"model\"]\n",
        "\n",
        "# Download the rdf.yaml files from all the folders\n",
        "yaml = YAML(typ=\"safe\")\n",
        "model_rdfs = [yaml.load(Path(pooch.retrieve(mu, known_hash=None))) for mu in model_urls]\n",
        "\n",
        "# Get only the models that have \"pytorch_state_dict\" weights\n",
        "pytorch_models = [rdf for rdf in model_rdfs if \"pytorch_state_dict\" in rdf[\"weights\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235264a0",
      "metadata": {
        "id": "235264a0",
        "tags": []
      },
      "source": [
        "## **2. Inspect a model from the BioImage Model Zoo**\n",
        "\n",
        "---\n",
        "\n",
        "Here we will guide you through the basic functionalities of the BioImageIO Python package to interact with the content in the BioImage Model Zoo.\n",
        "\n",
        "First, you can obtain a list of the available Bioimage Model Zoo models with PyTorch architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96d15d7",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c96d15d7",
        "outputId": "496e9642-bf45-4e4a-9ca0-90c22470b8cb"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check the models that can be loaded for PyTorch\n",
        "\n",
        "# Print all the PyTorch models (\"pytorch_state_dict\" weights) on the Bioimage Model Zoo\n",
        "print('List of models for PyTorch:\\n')\n",
        "for model in pytorch_models:\n",
        "    print(f\"{model['name']}\\n - {model['config']['bioimageio']['nickname']}\\n - {model['config']['bioimageio']['doi']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5162057-f0d1-419a-a127-2d6cabc5061b",
      "metadata": {
        "id": "f5162057-f0d1-419a-a127-2d6cabc5061b",
        "tags": []
      },
      "source": [
        "### **2.1. Load the resource description specifications of the model**\n",
        "\n",
        "---\n",
        "\n",
        "To load the model of your choice, you only need to fill one of the fields in the cell below and leave the rest empty. If more than one is filled the first one will be used.\n",
        "\n",
        "<font size = 3>**`BMZ_MODEL_ID`**: Unique identifier of the model to load in the BioImage Model Zoo, e.g., `impartial-shrimp`. These identifiers are available in each model card in the zoo.\n",
        "\n",
        "OR\n",
        "\n",
        "<font size = 3>**`BMZ_MODEL_DOI`**: Model DOIs can also be used to load the models.\n",
        "\n",
        "OR\n",
        "\n",
        "<font size = 3>**`BMZ_MODEL_URL`**: URL to the main Zenodo repository as well as to the `rdf.yaml` file containing the resource description specifications can also be used to load models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6745eebe-c01e-41ff-918d-41693a5d1526",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6745eebe-c01e-41ff-918d-41693a5d1526",
        "outputId": "4996095f-7ff0-4b58-dea9-ba3878fe0ca1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Load the model description with one of these options\n",
        "\n",
        "# \"affable-shark\"\n",
        "BMZ_MODEL_ID = \"affable-shark\" #@param {type:\"string\"}\n",
        "# \"10.5281/zenodo.5764892\"\n",
        "BMZ_MODEL_DOI = \"\" #@param {type:\"string\"}\n",
        "# \"https://uk1s3.embassy.ebi.ac.uk/public-datasets/bioimage.io/affable-shark/1.1/files/rdf.yaml\"\n",
        "BMZ_MODEL_URL = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#####\n",
        "# Load the model description from one of the provided options\n",
        "\n",
        "if BMZ_MODEL_ID != \"\":\n",
        "    model = load_description(BMZ_MODEL_ID)  # TODO: load from bioimageio id\n",
        "    print(f\"The model '{model.name}' with ID '{BMZ_MODEL_ID}' has been correctly loaded.\")\n",
        "elif BMZ_MODEL_DOI != \"\":\n",
        "    model = load_description(BMZ_MODEL_DOI)\n",
        "    print(f\"The model '{model.name}' with DOI '{BMZ_MODEL_DOI}' has been correctly loaded.\")\n",
        "elif BMZ_MODEL_URL != \"\":\n",
        "    model = load_description(BMZ_MODEL_URL)\n",
        "    print(f\"The model '{model.name}' with URL '{BMZ_MODEL_URL}' has been correctly loaded.\")\n",
        "else:\n",
        "    print('Please specify a model ID, DOI or URL')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07efa2a6-dc1e-46f6-989b-83d95bfaebbf",
      "metadata": {
        "id": "07efa2a6-dc1e-46f6-989b-83d95bfaebbf",
        "tags": []
      },
      "source": [
        "### **2.2. Discover the different components and features of the model**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZsE2Exb3PEZR",
      "metadata": {
        "id": "ZsE2Exb3PEZR"
      },
      "source": [
        "#### Print information about the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a35e70",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "12a35e70",
        "outputId": "d2c12619-ba90-4968-902e-7a18e7efc931",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Print information about the model\n",
        "\n",
        "print(f\"The model '{model.name} {model.id_emoji}' had the following properties and metadata:\")\n",
        "print()\n",
        "print(f\" Description: {model.description}\")\n",
        "print(f\" Tags: {', '.join(model.tags)}\")\n",
        "print(f\" Model ID: {model.id}\")\n",
        "print()\n",
        "print(f\" The authors of the model are:\")\n",
        "for author in model.authors:\n",
        "    print(f\"  - {author.name}, with GitHub user: @{author.github_user}\")\n",
        "print(f\" The maintainers of the modes are:\")\n",
        "for maintainer in model.maintainers:\n",
        "    print(f\"  - {maintainer.name}, with GitHub user: @{maintainer.github_user}\")\n",
        "print()\n",
        "print(f\" License: {model.license}\")\n",
        "print()\n",
        "print(f\" If you use this model, you are expected to cite:\")\n",
        "for citation in model.cite:\n",
        "    doi_text = \"\"\n",
        "    if citation.doi is not None:\n",
        "        doi_text = f\" from DOI (https://doi.org/{citation.doi})\"\n",
        "\n",
        "    url_text = \"\"\n",
        "    if citation.url is not None:\n",
        "        if citation.doi is not None:\n",
        "            url_text = f\" or URL ({citation.url})\"\n",
        "        else:\n",
        "            url_text = f\" from URL ({citation.url})\"\n",
        "    print(f\"  - For the {citation.text},{doi_text}{url_text}\")\n",
        "if model.git_repo is not None:\n",
        "    print()\n",
        "    print(f\" GitHub repository: {model.git_repo}\")\n",
        "print()\n",
        "print(f\" Covers of the model '{model.name}' are: \")\n",
        "for cover in model.covers:\n",
        "    cover_data = imread(download(cover).path)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cover_data)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "print()\n",
        "print(f\" Further documentation (taken from {model.documentation.absolute()}):\")\n",
        "print(' -'*60)\n",
        "display(Markdown(open(download(model.documentation).path).read()))\n",
        "print(' -'*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7_RhSnwTPIL_",
      "metadata": {
        "id": "7_RhSnwTPIL_"
      },
      "source": [
        "#### Inspect the weights, and expected inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579ee0e6-fb0f-4c2b-a025-6dfdc5518963",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "579ee0e6-fb0f-4c2b-a025-6dfdc5518963",
        "outputId": "88afd6b5-b751-48c7-d86d-2c4a45a466cb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Inspect the weights, and expected inputs and outputs\n",
        "\n",
        "print(\"Available weight formats for this model:\", \", \".join(model.weights.model_fields_set))\n",
        "print(\"Pytorch state dict weights are stored at:\", model.weights.pytorch_state_dict.source.absolute())\n",
        "print()\n",
        "\n",
        "# or what inputs the model expects\n",
        "print(f\"The model requires {len(model.inputs)} input(s) with the following features:\")\n",
        "for inp in model.inputs:\n",
        "    print(\" - Input with axes:\", ([i.id for i in inp.axes]))\n",
        "    print(\" - Minimum shape:\", ([s.min if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in inp.shape]))\n",
        "    print(\" - Step:\", ([s.step if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in inp.shape]))\n",
        "    print()\n",
        "    print(f\"It is expected to be pre-processed with:\")\n",
        "    for prep in inp.preprocessing:\n",
        "        print(f\" - '{prep.id}' with arguments:\")\n",
        "        for prerp_arg in prep.kwargs:\n",
        "           print(f\"    - {prerp_arg[0]}={prerp_arg[1]}\")\n",
        "print()\n",
        "\n",
        "# and what the model outputs are\n",
        "print(f\"The model gives {len(model.outputs)} output(s) with the following features:\")\n",
        "for out in model.outputs:\n",
        "    print(\" - Output with axes:\", ([o.id for o in out.axes]) )\n",
        "    print(\" - Minimum shape:\", ([s if type(s) is bioimageio.spec.model.v0_5.SizeReference else s for s in out.shape]))\n",
        "    print(\" - Step:\", ([s.step if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize else s for s in out.shape]))\n",
        "    print()\n",
        "    print(f\"It is expected to be post-processed with:\")\n",
        "    for postp in out.postprocessing:\n",
        "        print(f\" - '{postp.id}' with arguments:\")\n",
        "        for postp_arg in postp.kwargs:\n",
        "           print(f\"    - {postp_arg[0]}={postp_arg[1]}\")\n",
        "    #print(f\"The output image has a halo of : {out.halo}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fkz_oROmQNui",
      "metadata": {
        "id": "Fkz_oROmQNui"
      },
      "source": [
        "#### Inspect the test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00bef6db-a11e-4bf2-bb2b-d2026df35801",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "00bef6db-a11e-4bf2-bb2b-d2026df35801",
        "outputId": "a56b37b0-8495-4fcc-9912-9b2d682e5202",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Inspect the test images\n",
        "\n",
        "# Inspect the test input images provided with the model\n",
        "print(f\"The model provides {len(model.inputs)} test input image(s) :\")\n",
        "for test_im in model.get_input_test_arrays():\n",
        "    test_input = np.squeeze(test_im)\n",
        "    if len(test_input.shape)>2:\n",
        "        print(f\"The test input image has shape {test_input.shape}, so it will not displayed\")\n",
        "    else:\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(test_input, cmap=\"afmhot\")\n",
        "        fig.colorbar(im)\n",
        "        ax.set_axis_off()\n",
        "        plt.show()\n",
        "\n",
        "# Inspect the test output images provided with the model\n",
        "print(f\"The model provides {len(model.outputs)} test output image(s) :\")\n",
        "for test_im in model.get_output_test_arrays():\n",
        "    test_output = np.squeeze(test_im)\n",
        "    if len(test_output.shape)>2:\n",
        "        print(f\"The test output image has shape {test_output.shape}, so it will not displayed\")\n",
        "    else:\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(test_output, cmap=\"afmhot\")\n",
        "        fig.colorbar(im)\n",
        "        ax.set_axis_off()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5df5f7c-2ce0-4230-b30f-682a6aedf84a",
      "metadata": {
        "id": "f5df5f7c-2ce0-4230-b30f-682a6aedf84a"
      },
      "source": [
        "## **3. Test the model**\n",
        "\n",
        "---\n",
        "\n",
        "Both the model format and the deployment of the model can be tested.\n",
        "\n",
        "By running the following cell you can check that\n",
        "- The model follows the format of the BioImage Model Zoo correctly (static validation)\n",
        "- It actually produces the output that is expected to produce (dynamic validation). This is done by running a prediction for the test input images and checking that they agree with the given test output(s).\n",
        "\n",
        "The running time depends on the resources available (e.g., GPU acceleration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c805d5e7",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c805d5e7",
        "outputId": "8aacc5e5-642e-49b6-9e3f-b2d475e04357",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check if the model passes the test\n",
        "\n",
        "# Test the description of the model\n",
        "test_result = test_model(model)\n",
        "\n",
        "# 'test_model()' returns a ValidationSummary object with an attribute status which can be 'passed'/'failed' and more detailed information\n",
        "if test_result.status == \"failed\":\n",
        "    print(\"model test:\", test_result.name)\n",
        "    if len(test_result.errors) > 1:\n",
        "        print(\"The model test failed with many errors. We will only show the first one:\")\n",
        "    else:\n",
        "        print(\"The model test failed with:\")\n",
        "    for error in test_result.errors[:1]:\n",
        "        # Allowing a good indexation in the message\n",
        "        error_msg = error.msg.split('\\n')\n",
        "        print(f\" - {error_msg[0]}\")\n",
        "        for line in error_msg[1:]:\n",
        "            print(f\"   {line}\")\n",
        "else:\n",
        "    print(\"model test:\", test_result.name)\n",
        "    assert test_result.status == \"passed\", f\"Something went wrong, test_result.status is {test_result.status} and should be 'passed'.\"\n",
        "    print(\"The model passed the test.\")\n",
        "    print()\n",
        "\n",
        "# Get the versions of the Bioimage.IO packages used for this test\n",
        "bioimageio_spec_version = \"\"\n",
        "bioimageio_core_version = \"\"\n",
        "for package in test_result.env:\n",
        "    if package['name'] == 'bioimageio.spec':\n",
        "        bioimageio_spec_version = package['version']\n",
        "    if package['name'] == 'bioimageio.core':\n",
        "        bioimageio_core_version = package['version']\n",
        "\n",
        "print()\n",
        "print(f\"The model was tested using:\")\n",
        "print(f\" - 'bioimageio_spec_version': '{bioimageio_spec_version}'\")\n",
        "print(f\" - 'bioimageio_core_version': '{bioimageio_core_version}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8402e3a5-9093-45f0-8d09-1c2c8b1fdc53",
      "metadata": {
        "id": "8402e3a5-9093-45f0-8d09-1c2c8b1fdc53"
      },
      "source": [
        "## **4. Use the model with new images**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda17596-9fb4-41e8-aef1-e6c4eb2da579",
      "metadata": {
        "id": "eda17596-9fb4-41e8-aef1-e6c4eb2da579",
        "tags": []
      },
      "source": [
        "### **4.1. Process the example input array**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b37a8a-7ee2-417c-92d9-92a5372e3835",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "46b37a8a-7ee2-417c-92d9-92a5372e3835",
        "outputId": "91522f8e-9dcd-482f-8223-33ab312de8b0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Process the example input within the model\n",
        "\n",
        "# Load the example image for this model\n",
        "input_paths = {ipt.id: download(ipt.test_tensor).path for ipt in model.inputs}\n",
        "# The prediction pipeline expects a Sample object from bioimageio.core\n",
        "input_sample = create_sample_for_model(\n",
        "    model=model, inputs=input_paths, sample_id=\"my_demo_sample\"\n",
        ")\n",
        "\n",
        "# \"devices\" can be used to run prediction on a gpu instead of the cpu\n",
        "devices = None\n",
        "# \"weight_format\" to specify which weight format to use in case the model contains different weight formats\n",
        "weight_format = None\n",
        "\n",
        "# The prediction pipeline combines preprocessing, prediction and postprocessing.\n",
        "# It should always be used for prediction with a bioimageio model.\n",
        "prediction_pipeline = create_prediction_pipeline(\n",
        "    model, devices=devices, weight_format=weight_format\n",
        ")\n",
        "\n",
        "# The prediction pipeline call expects the same number of inputs as the number of inputs required by the model\n",
        "# In this case, the model just expects a single input. In case you have multiple inputs use:\n",
        "# prediction = pred_pipeline(input1, input2, ...)\n",
        "# or, if you have the inputs in a list or tuple\n",
        "# prediction = pred_pipeline(*inputs)\n",
        "# The call returns a list of output tensors, corresponding to the output tensors of the model\n",
        "prediction = prediction_pipeline.predict_sample_without_blocking(input_sample)\n",
        "\n",
        "# Convert both input and output sample tensors into a NumPy format to plot them\n",
        "input_sample_tensor = input_sample.members[\"input0\"].data\n",
        "input_sample_tensor = np.squeeze(input_sample_tensor)\n",
        "prediction_tensor = prediction.members[\"output0\"].data\n",
        "prediction_tensor = np.squeeze(prediction_tensor)\n",
        "\n",
        "# Plot the input and output images\n",
        "if len(prediction_tensor.shape)>2:\n",
        "    subplot_n = prediction_tensor.shape[0]\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.subplot(1,1+subplot_n,1)\n",
        "    plt.imshow(input_sample_tensor, cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input image to process\")\n",
        "\n",
        "    for i in range(subplot_n):\n",
        "        plt.subplot(1,1+subplot_n,i+2)\n",
        "        plt.imshow(prediction_tensor[i], cmap=\"afmhot\")\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Processed image\")\n",
        "    plt.show()\n",
        "else:\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(input_sample_tensor, cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input image to process\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(prediction_tensor, cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Processed image\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43854569-1e3a-443c-9e5e-fc2b97f26480",
      "metadata": {
        "cellView": "form",
        "id": "43854569-1e3a-443c-9e5e-fc2b97f26480",
        "tags": []
      },
      "source": [
        "### **4.2. Process a single image and save the result**\n",
        "\n",
        "---\n",
        "\n",
        "The BioImageIO core library is equipped with the utility function `predict` to run predictions on an image stored in disk. It accepts most common image formats (`.tif`, `.png`) as well as `npy` fileformat as inputs, and the output prediction can be stored in a local `Results_folder` directory.\n",
        "\n",
        "Provide the path to the image to be processed in `Image_path` or run it with the example image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad3a2f4",
      "metadata": {
        "cellView": "form",
        "id": "3ad3a2f4",
        "outputId": "a56f4045-587e-4d9a-aacb-5e1262161efb"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Indicate the path to the image\n",
        "\n",
        "# You might want to use the test images (like on section 4.1.)\n",
        "use_test_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### If you have an image in a folder to segment, copy the path to it here:\n",
        "Image_path = \"\"  #@param {type:\"string\"}\n",
        "#@markdown ### Indicate where to save the output of the model:s\n",
        "Results_folder = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "# Create the result/output folder\n",
        "os.makedirs(Results_folder, exist_ok=True)\n",
        "\n",
        "if use_test_image:\n",
        "  # Download and take the input paths from the model description\n",
        "  input_paths = {ipt.id: download(ipt.test_tensor).path for ipt in model.inputs}\n",
        "else:\n",
        "  # Load the paths to the input images\n",
        "  input_paths = {\"input0\": Path(Image_path)}\n",
        "\n",
        "# The prediction pipeline expects a Sample object from bioimageio.core\n",
        "input_sample = create_sample_for_model(\n",
        "    model=model, inputs=input_paths, sample_id=\"my_demo_sample\"\n",
        ")\n",
        "\n",
        "# Use the predict function with the defined Samples\n",
        "prediction = predict(model=model, inputs=input_sample)\n",
        "\n",
        "# Convert both input and output sample tensors into a NumPy format to save and plot them\n",
        "input_sample_tensor = input_sample.members[\"input0\"].data\n",
        "input_sample_tensor = np.squeeze(input_sample_tensor)\n",
        "prediction_tensor = prediction.members[\"output0\"].data\n",
        "prediction_tensor = np.squeeze(prediction_tensor)\n",
        "\n",
        "# Save the output results\n",
        "filename = Image_path.split(os.sep)[-1]\n",
        "name, extension = filename.split('.')\n",
        "for i, f in enumerate(prediction_tensor):\n",
        "  imsave(uri=os.path.join(Results_folder, f\"{name}_{i}.{extension}\"), im=f)\n",
        "print(f\"Predicted images correctly saved on: {Results_folder}\")\n",
        "\n",
        "# Plot the input image and the predicted results\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,1+prediction_tensor.shape[0],1)\n",
        "plt.imshow(np.squeeze(input_sample_tensor), cmap=\"afmhot\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Input image to process\")\n",
        "\n",
        "for pred_idx, pred in enumerate(prediction_tensor):\n",
        "    plt.subplot(1,1+prediction_tensor.shape[0],pred_idx+2)\n",
        "    plt.imshow(pred, cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Processed image {pred_idx+1}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c9cf63-56e6-4f15-9ae6-bfdc5f99620f",
      "metadata": {
        "id": "e2c9cf63-56e6-4f15-9ae6-bfdc5f99620f",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### **4.3. Process all images stored in a directory**\n",
        "\n",
        "---\n",
        "It is possible to provide a list of images to analyse and run the prediction for each automatically. \n",
        "\n",
        "In this example `tiling`, or `blocking`, strategies can be enabled. This divides the image into smaller patches, each of which is processed independently, and then rejoined.\n",
        "\n",
        "`Tile_size` must be equal or lower than the input image size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774b95c7-3ab5-4c58-a6c9-d07179caf036",
      "metadata": {
        "cellView": "form",
        "id": "774b95c7-3ab5-4c58-a6c9-d07179caf036",
        "outputId": "1a179fc5-e552-45f8-aa88-99a931758e51",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@markdown ##Indicate a directory with images to analyse and a directory to save the images\n",
        "\n",
        "Image_path = \"\" #@param {type:\"string\"}\n",
        "Results_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "Tiling = True #@param {type:\"boolean\"}\n",
        "Tile_size = 256 #@param {type:\"integer\"}\n",
        "\n",
        "# Create the result/output folder\n",
        "#os.makedirs(Results_folder, exist_ok=True)\n",
        "\n",
        "# Calculate the blocksize parameter from the Tile_size\n",
        "if Tiling:\n",
        "    inp = model.inputs[0]\n",
        "    # Get the minimum block size and the block step values from the model description RDF\n",
        "    min_block_size = max([s.min for s in inp.shape if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize])\n",
        "    block_step = max([s.step for s in inp.shape if type(s) is bioimageio.spec.model.v0_5.ParameterizedSize])\n",
        "    blocksize_parameter = (Tile_size - min_block_size) // block_step\n",
        "        \n",
        "\n",
        "# Get the list of images to analyse and the same list to save the images\n",
        "input_path_list = [os.path.join(Image_path,f) for f in sorted(os.listdir(Image_path)) if not f.startswith('.')]\n",
        "\n",
        "print(f\"Provided directory contains {len(input_path_list)} images to predict.\")\n",
        "\n",
        "for in_idx, in_path in enumerate(input_path_list):\n",
        "\n",
        "    # Apparently the implementation requires to provide an input sample of the same shape and\n",
        "    # with the same id as the one the model already ahs, for that reason we need to process images\n",
        "    # one by one.\n",
        "    input_paths_dict = {f\"input0\": Path(in_path)}\n",
        "\n",
        "    # The prediction pipeline expects a Sample object from bioimageio.core\n",
        "    input_sample = create_sample_for_model(\n",
        "        model=model, inputs=input_paths_dict, sample_id=\"my_demo_sample\"\n",
        "    )\n",
        "\n",
        "    # Use the predict function with the defined Samples, with or without tiling\n",
        "    if not Tiling:\n",
        "        prediction = predict(model=model, inputs=input_sample)\n",
        "    else:\n",
        "        prediction = predict(model=model, inputs=input_sample, blocksize_parameter=blocksize_parameter)\n",
        "\n",
        "    # Convert both input and output sample tensors into a NumPy format to save and plot them\n",
        "    input_sample_tensor = input_sample.members[\"input0\"].data\n",
        "    input_sample_tensor = np.squeeze(input_sample_tensor)\n",
        "    prediction_tensor = prediction.members[\"output0\"].data\n",
        "    prediction_tensor = np.squeeze(prediction_tensor)\n",
        "\n",
        "    # Save the output results\n",
        "    filename = in_path.split(os.sep)[-1]\n",
        "    name, extension = filename.split('.')\n",
        "    for pred_idx, pred in enumerate(prediction_tensor):\n",
        "        imsave(uri=os.path.join(Results_folder, f\"{name}_{pred_idx}.{extension}\"), im=pred)\n",
        "\n",
        "    # Plot the input image and the predicted results\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,1+prediction_tensor.shape[0],1)\n",
        "    plt.imshow(np.squeeze(input_sample_tensor), cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Input image to process\")\n",
        "\n",
        "    for pred_idx, pred in enumerate(prediction_tensor):\n",
        "        plt.subplot(1,1+prediction_tensor.shape[0],pred_idx+2)\n",
        "        plt.imshow(pred, cmap=\"afmhot\")\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Processed image {pred_idx+1}\")\n",
        "    plt.show()\n",
        "\n",
        "print(f\"Predicted images correctly saved on: {Results_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UnYCJehOiSSF",
      "metadata": {
        "id": "UnYCJehOiSSF"
      },
      "source": [
        "## **5. Fine-tune an existing model (only for segmentation)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UM-fC1nwNQr2",
      "metadata": {
        "id": "UM-fC1nwNQr2"
      },
      "source": [
        "### **5.1. Load the training functions**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5iczcnVViOJt",
      "metadata": {
        "cellView": "form",
        "id": "5iczcnVViOJt"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Run the following cell to load the training functions\n",
        "\n",
        "from marshmallow import missing\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MultiLabelSoftMarginLoss\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "def load_pytorch_model(model):\n",
        "    weight_spec = model.weights.pytorch_state_dict\n",
        "    model_kwargs = weight_spec.architecture.kwargs\n",
        "    joined_kwargs = {} if model_kwargs is missing else dict(model_kwargs)\n",
        "\n",
        "    # Download the Python file with the model\n",
        "    model_fullpath = str(download(model.weights.pytorch_state_dict.architecture.source).path)\n",
        "    model_path, model_filename = os.path.split(model_fullpath)\n",
        "\n",
        "    # Add it to the sys.path list\n",
        "    sys.path.insert(0, str(model_path))\n",
        "\n",
        "    # Get the name of the model's class\n",
        "    module_name = str(model.weights.pytorch_state_dict.architecture.callable)\n",
        "\n",
        "    # Import the Python file\n",
        "    spec = importlib.util.spec_from_file_location(module_name, model_fullpath)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "\n",
        "    # Add the module to sys.modules\n",
        "    sys.modules[module_name] = module\n",
        "\n",
        "    # Import the model\n",
        "    model_class = getattr(importlib.import_module(module_name), module_name)\n",
        "    # Initialzie the model\n",
        "    model_instance = model_class(**joined_kwargs)\n",
        "\n",
        "    print(f\"Model {module_name} succesfully initialized!\")\n",
        "\n",
        "    _devices = [\"cuda\" if torch.cuda.is_available() else \"cpu\"]\n",
        "\n",
        "    print(_devices)\n",
        "    if len(_devices) > 1:\n",
        "        warnings.warn(\"Multiple devices for single pytorch model not yet implemented\")\n",
        "    model_instance.to(_devices[0])\n",
        "\n",
        "    weights= model.weights.pytorch_state_dict\n",
        "\n",
        "    if weights is not None and weights.source:\n",
        "        weights_fullpath = str(download(weights.source).path)\n",
        "        state = torch.load(weights_fullpath, map_location=_devices[0])\n",
        "        model_instance.load_state_dict(state)\n",
        "    model_instance.eval()\n",
        "\n",
        "    return model_instance\n",
        "\n",
        "\n",
        "class SegmentationTrainDataset(Dataset):\n",
        "    def __init__(self,  INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths, maskPaths, classes):\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.classes = classes\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        return len(self.imagePaths)\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        norm_im = np.float32(im)\n",
        "        return (norm_im - np.mean(norm_im)) / (np.std(norm_im) + 1.0e-6)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the image path from the current index\n",
        "        image = imread(self.imagePaths[idx]) \n",
        "        mask = imread(self.maskPaths[idx])\n",
        "        # we want to ensure that there are cells in the patch\n",
        "        # without getting in an infinite loop\n",
        "        # TODO: define a sampling function to remove the loop\n",
        "        num_labels = 0\n",
        "        k = 0\n",
        "        while num_labels<(self.classes-1) and k<5:\n",
        "          # Choose a random coordinate to crop a patch\n",
        "          h = random.randint(1, image.shape[0]-INPUT_IMAGE_HEIGHT-1)\n",
        "          w = random.randint(1, image.shape[1]-INPUT_IMAGE_WIDTH-1)\n",
        "          mask_patch = mask[h:h+INPUT_IMAGE_HEIGHT, w:w+INPUT_IMAGE_WIDTH]\n",
        "          num_labels = len(np.unique(mask_patch))\n",
        "          # If the mask contains more than one label for semantic segmentation\n",
        "          # we will trasnform into one-hot encoding\n",
        "          mask_torch = torch.tensor(mask_patch).to(torch.int64)\n",
        "          mask_hot = one_hot(mask_torch, self.classes)\n",
        "          mask_hot = mask_hot[:,:,1:]\n",
        "          if len(mask_hot.shape)==2:\n",
        "            # add a dimension\n",
        "            mask_hot = np.expand_dims(mask_hot, -1)\n",
        "          # first axis goes to the channels\n",
        "          mask_hot = np.transpose(mask_hot, [-1, 0, 1])\n",
        "          k += 1\n",
        "          # return a tuple of the image and its mask\n",
        "        norm_image = self.preprocess(image)\n",
        "        norm_image = np.expand_dims(norm_image[h:h+INPUT_IMAGE_HEIGHT, w:w+INPUT_IMAGE_WIDTH], 0)\n",
        "        return (torch.tensor(norm_image).float(), torch.tensor(mask_hot).float())\n",
        "\n",
        "class SegmentationTestDataset(Dataset):\n",
        "    def __init__(self,  INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths, maskPaths, classes):\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.classes = classes\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        return len(self.imagePaths)\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        norm_im = np.float32(im)\n",
        "        return (norm_im - np.mean(norm_im)) / (np.std(norm_im) + 1.0e-6)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the image path from the current index\n",
        "        image = imread(self.imagePaths[idx])\n",
        "        mask = imread(self.maskPaths[idx])\n",
        "        # no patches are cropped. Check for the memory\n",
        "        mask_torch = torch.tensor(mask).to(torch.int64)\n",
        "        mask_hot = one_hot(mask_torch, self.classes)\n",
        "        mask_hot = mask_hot[:,:,1:]\n",
        "        if len(mask_hot.shape)==2:\n",
        "          # add a dimension\n",
        "          mask_hot = np.expand_dims(mask_hot, -1)\n",
        "        # first axis goes to the channels\n",
        "        mask_hot = np.transpose(mask_hot, [-1, 0, 1])\n",
        "        # return a tuple of the image and its mask\n",
        "        norm_image = self.preprocess(image)\n",
        "        norm_image = np.expand_dims(norm_image, 0)\n",
        "        return (torch.tensor(norm_image).float(), torch.tensor(mask_hot).float())\n",
        "\n",
        "\n",
        "def visualize_results(input_image, gt, prediction):\n",
        "    if len(prediction.shape)>2:\n",
        "        subplot_n = prediction.shape[0]\n",
        "        plt.figure(figsize=(20,15))\n",
        "        plt.subplot(subplot_n,3,1)\n",
        "        plt.imshow(np.squeeze(input_image), cmap=\"gray\")\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Input test image\")\n",
        "\n",
        "        for i in range(subplot_n):\n",
        "            plt.subplot(subplot_n,3,i*3+2)\n",
        "            plt.imshow(np.squeeze(gt[i]), cmap=\"afmhot\")\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Ground truth image\")\n",
        "\n",
        "            plt.subplot(subplot_n,3,i*3+3)\n",
        "            plt.imshow(np.squeeze(prediction[i]), cmap=\"afmhot\")\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Processed image\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(np.squeeze(input_image), cmap=\"afmhot\")\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Input test image\")\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(np.squeeze(gt), cmap=\"afmhot\")\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Ground truth image\")\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(np.squeeze(prediction), cmap=\"afmhot\")\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Processed image\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def finetune_bioimageio_model(model, TRAIN_IM, TRAIN_MASK, TEST_IM, TEST_MASK,\n",
        "                              BASE_OUTPUT, NUM_EPOCHS=100, INIT_LR=0.0001, BATCH_SIZE=10,\n",
        "                              INPUT_IMAGE_WIDTH=512, INPUT_IMAGE_HEIGHT=512, CLASSES=3):\n",
        "\n",
        "    model_instance = load_pytorch_model(model)\n",
        "    # create the train and test datasets\n",
        "    trainImages = sorted([os.path.join(TRAIN_IM, i) for i in os.listdir(TRAIN_IM) if i.endswith(\".tif\") ])\n",
        "    trainMasks = sorted([os.path.join(TRAIN_MASK, i) for i in os.listdir(TRAIN_MASK) if i.endswith(\".tif\") ])\n",
        "    testImages = sorted([os.path.join(TEST_IM, i) for i in os.listdir(TEST_IM) if i.endswith(\".tif\") ])\n",
        "    testMasks = sorted([os.path.join(TEST_MASK, i) for i in os.listdir(TEST_MASK) if i.endswith(\".tif\") ])\n",
        "\n",
        "    trainDS = SegmentationTrainDataset( INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths=trainImages, maskPaths=trainMasks, classes = CLASSES)\n",
        "    testDS = SegmentationTestDataset( INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT, imagePaths=testImages, maskPaths=testMasks, classes = CLASSES)\n",
        "    print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
        "    print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
        "\n",
        "    # create the training and test data loaders\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # determine the device to be used for training and evaluation\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # determine if we will be pinning memory during data loading\n",
        "    PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
        "    trainLoader = DataLoader(trainDS, shuffle=True, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "    testLoader = DataLoader(testDS, shuffle=False, batch_size=1, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "\n",
        "    # initialize loss function and optimizer\n",
        "    lossFunc = CrossEntropyLoss()\n",
        "    opt = Adam(model_instance.parameters(), lr=INIT_LR)\n",
        "    # calculate steps per epoch for training and test set\n",
        "    trainSteps = len(trainDS) // BATCH_SIZE\n",
        "    testSteps = len(testDS)\n",
        "    # initialize a dictionary to store training history\n",
        "    H = {\"train_loss\": [], \"test_loss\": []}\n",
        "\n",
        "    # Save the initial prediction\n",
        "    x, y = testDS.__getitem__(-1)\n",
        "    x = x.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        # set the model in evaluation mode\n",
        "        model_instance.eval()\n",
        "        # send the input to the device\n",
        "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "        # make the predictions\n",
        "        pred = model_instance(x)\n",
        "\n",
        "    input_image = x.to(\"cpu\")\n",
        "    input_image = np.squeeze(input_image.numpy())\n",
        "\n",
        "    gt = y.to(\"cpu\")\n",
        "    gt = np.squeeze(gt.numpy())\n",
        "\n",
        "    prediction = pred.to(\"cpu\")\n",
        "    prediction = np.squeeze(prediction.numpy())\n",
        "    print(\"Results of the prediction before finetuning\")\n",
        "    print(\"---------------------------------------------\")\n",
        "    visualize_results(input_image, gt, prediction)\n",
        "    print(\"---------------------------------------------\")\n",
        "    del x, y, gt, prediction, input_image, pred\n",
        "\n",
        "    # loop over epochs\n",
        "    print(\"[INFO] training the network...\")\n",
        "    startTime = time.time()\n",
        "    for e in tqdm(range(NUM_EPOCHS)):\n",
        "        # set the model in training mode\n",
        "        model_instance.train()\n",
        "        # initialize the total training and validation loss\n",
        "        totalTrainLoss = 0\n",
        "        totalTestLoss = 0\n",
        "        # loop over the training set\n",
        "\n",
        "        for (i, (x, y)) in enumerate(trainLoader):\n",
        "            # send the input to the device\n",
        "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "            # perform a forward pass and calculate the training loss\n",
        "            pred = model_instance(x)\n",
        "            loss = lossFunc(pred, y)\n",
        "            # first, zero out any previously accumulated gradients, then\n",
        "            # perform backpropagation, and then update model parameters\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            # add the loss to the total training loss so far\n",
        "            totalTrainLoss += loss\n",
        "        # switch off autograd\n",
        "        with torch.no_grad():\n",
        "            # set the model in evaluation mode\n",
        "            model_instance.eval()\n",
        "            # loop over the validation set\n",
        "            for (x, y) in testLoader:\n",
        "                # send the input to the device\n",
        "                (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "                # make the predictions and calculate the validation loss\n",
        "                pred = model_instance(x)\n",
        "                totalTestLoss += lossFunc(pred, y)\n",
        "        if DEVICE==\"cuda\":\n",
        "            torch.save(model_instance.state_dict(),\n",
        "                       os.path.join(BASE_OUTPUT, \"finetuned_last.pth\"))\n",
        "        else:\n",
        "            torch.save(model_instance.cpu().state_dict(),\n",
        "                       os.path.join(BASE_OUTPUT, \"finetuned_last.pth\"))\n",
        "\n",
        "        # calculate the average training and validation loss\n",
        "        avgTrainLoss = totalTrainLoss / trainSteps\n",
        "        avgTestLoss = totalTestLoss / testSteps\n",
        "        # update our training history\n",
        "        H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "        H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
        "        # print the model training and validation information\n",
        "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "        print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
        "            avgTrainLoss, avgTestLoss))\n",
        "    # display the total time needed to perform the training\n",
        "    endTime = time.time()\n",
        "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
        "\n",
        "    input_image = x.to(\"cpu\")\n",
        "    input_image = np.squeeze(input_image.numpy())\n",
        "\n",
        "    gt = y.to(\"cpu\")\n",
        "    gt = np.squeeze(gt.numpy())\n",
        "\n",
        "    prediction = pred.to(\"cpu\")\n",
        "    prediction = np.squeeze(prediction.numpy())\n",
        "\n",
        "    visualize_results(input_image, gt, prediction)\n",
        "\n",
        "    return model_instance, H\n",
        "\n",
        "# get the python file defining the architecture.\n",
        "# this is only required for models with pytorch_state_dict weights\n",
        "def get_architecture_source(rdf):\n",
        "    import bioimageio\n",
        "    # here, we need the raw resource, which contains the information from the resource description\n",
        "    # before evaluation, e.g. the file and name of the python file with the model architecture\n",
        "    raw_resource = bioimageio.core.load_raw_resource_description(rdf)\n",
        "    # the python file defining the architecture for the pytorch weihgts\n",
        "    model_source = raw_resource.weights[\"pytorch_state_dict\"].architecture\n",
        "    # download the source file if necessary\n",
        "    source_file = bioimageio.core.resource_io.utils.resolve_source(\n",
        "        model_source.source_file\n",
        "    )\n",
        "    # if the source file path does not exist, try combining it with the root path of the model\n",
        "    if not os.path.exists(source_file):\n",
        "        source_file = os.path.join(raw_resource.root_path, os.path.split(source_file)[1])\n",
        "    assert os.path.exists(source_file), source_file\n",
        "    class_name = model_source.callable_name\n",
        "    return f\"{source_file}:{class_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72600171",
      "metadata": {
        "id": "72600171"
      },
      "source": [
        "### **5.2. Start the fine-tuning**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RjcVR1RYR1N1",
      "metadata": {
        "id": "RjcVR1RYR1N1"
      },
      "source": [
        "##### Run the following cell to visualize the results of the pretrained model on the new images before running the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X7hNpKpAL7lg",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "X7hNpKpAL7lg",
        "outputId": "2d03cdf8-873b-4018-c14f-fa49b1378e9b"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Indicate the path to the image\n",
        "\n",
        "# You might want to use the test images (like on section 4.1.)\n",
        "use_test_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### If you have an image in a folder to segment, copy the path to it here:\n",
        "Image_path = \"\"  #@param {type:\"string\"}\n",
        "#@markdown ### Indicate where to save the output of the model:\n",
        "Results_folder = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "if use_test_image:\n",
        "  # Download and take the input paths from the model description\n",
        "  input_paths = {ipt.id: download(ipt.test_tensor).path for ipt in model.inputs}\n",
        "else:\n",
        "  # Load the paths to the input images\n",
        "  input_paths = {\"input0\": Path(Image_path)}\n",
        "\n",
        "# The prediction pipeline expects a Sample object from bioimageio.core\n",
        "input_sample = create_sample_for_model(\n",
        "    model=model, inputs=input_paths, sample_id=\"my_demo_sample\"\n",
        ")\n",
        "\n",
        "# The prediction pipeline expects a Sample object from bioimageio.core\n",
        "prediction = predict(model=model, inputs=input_sample)\n",
        "\n",
        "# Convert both input and output sample tensors into a NumPy format to save and plot them\n",
        "input_sample_tensor = input_sample.members[\"input0\"].data\n",
        "input_sample_tensor = np.squeeze(input_sample_tensor)\n",
        "prediction_tensor = prediction.members[\"output0\"].data\n",
        "prediction_tensor = np.squeeze(prediction_tensor)\n",
        "\n",
        "# Save the output results\n",
        "filename = Image_path.split(os.sep)[-1]\n",
        "name, extenstion = filename.split('.')\n",
        "for i, f in enumerate(prediction_tensor):\n",
        "  imsave(uri=os.path.join(Results_folder, f\"{name}_{i}.{extenstion}\"), im=f)\n",
        "\n",
        "# Plot the input image and the predicted results\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,1+prediction_tensor.shape[0],1)\n",
        "plt.imshow(np.squeeze(input_sample_tensor), cmap=\"afmhot\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Input image to process\")\n",
        "\n",
        "for pred_idx, pred in enumerate(prediction_tensor):\n",
        "    plt.subplot(1,1+prediction_tensor.shape[0],pred_idx+2)\n",
        "    plt.imshow(pred, cmap=\"afmhot\")\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Processed image {pred_idx+1}\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kj8oT2FCR-7W",
      "metadata": {
        "id": "Kj8oT2FCR-7W"
      },
      "source": [
        "##### Run the following cell to set up the parameters for the fine-tuning and run it.\n",
        "\n",
        "The fine-tunning requires the following parameters:\n",
        "- Initial learning rate `INIT_LR`, number of training epochs `NUM_EPOCHS`, and batch size `BATCH_SIZE`\n",
        "- The input image dimensions `INPUT_IMAGE_WIDTH` and `INPUT_IMAGE_HEIGHT`, and number of label classes `NUM_CLASSES` (including background)\n",
        "- Paired folders of Images and Masks, one for training and one for validation. \n",
        "- An output directory `BASE_OUTPUT`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1OT8OBediZeK",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1OT8OBediZeK",
        "outputId": "65721b94-e5ed-440d-85b3-b6c6055cd72c"
      },
      "outputs": [],
      "source": [
        "# @markdown ## Set up training parameters\n",
        "\n",
        "# @markdown Initialize learning rate, number of training epochs, and batch size\n",
        "INIT_LR = 0.000001 #@param {type:\"number\"}\n",
        "NUM_EPOCHS = 200 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 20 #@param {type:\"integer\"}\n",
        "# @markdown Shape of the input images and number of classes\n",
        "INPUT_IMAGE_WIDTH = 256 #@param {type:\"integer\"}\n",
        "INPUT_IMAGE_HEIGHT = 256 #@param {type:\"integer\"}\n",
        "CLASSES = 3 #@param {type:\"integer\"}\n",
        "# @markdown Path to the training data paired folders\n",
        "TRAIN_IM = \"\" #@param {type:\"string\"}\n",
        "TRAIN_MASK = \"\" #@param {type:\"string\"}\n",
        "# @markdown Path to the test data paired folders\n",
        "TEST_IM = \"\" #@param {type:\"string\"}\n",
        "TEST_MASK = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Path to the directory to save the new model\n",
        "# Define the path to the base output directory\n",
        "BASE_OUTPUT = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Make the output directory\n",
        "os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
        "\n",
        "# Define the path to the output serialized model, model training\n",
        "# plot, and testing image paths\n",
        "MODEL_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio.pth\")\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
        "\n",
        "## -----\n",
        "## Release some memory\n",
        "# del model_instance\n",
        "# del finetuned_model\n",
        "torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "## -----\n",
        "\n",
        "finetuned_model, H = finetune_bioimageio_model(model, TRAIN_IM, TRAIN_MASK, TEST_IM, TEST_MASK,\n",
        "                                          BASE_OUTPUT, NUM_EPOCHS=NUM_EPOCHS, INIT_LR=INIT_LR, BATCH_SIZE=BATCH_SIZE,\n",
        "                                          INPUT_IMAGE_WIDTH=INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT=INPUT_IMAGE_HEIGHT,\n",
        "                                               CLASSES=CLASSES)\n",
        "\n",
        "## Save the model in two different formats\n",
        "\n",
        "# Save the model as a pytorch statedict\n",
        "MODEL_STATEDICT_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio_statedict_model.pth\")\n",
        "torch.save(finetuned_model.cpu().state_dict(),MODEL_STATEDICT_PATH)\n",
        "\n",
        "# Convert the model to a torchscript format and save it as torchscript\n",
        "MODEL_TORCHSCRIPT_PATH = os.path.join(BASE_OUTPUT, \"finetuned_bioimageio_torchscript_model.pt\")\n",
        "with torch.no_grad():\n",
        "    # load input and expected output data\n",
        "    input_data = [np.load(download(inp.test_tensor.source).path).astype(\"float32\") for inp in model.inputs]\n",
        "    input_data = [torch.from_numpy(inp) for inp in input_data]\n",
        "    scripted_model = torch.jit.trace(finetuned_model.cpu(), input_data)\n",
        "    scripted_model.save(MODEL_TORCHSCRIPT_PATH)\n",
        "\n",
        "# Plot the training loss and tr=est loss\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(PLOT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56d6ebf",
      "metadata": {
        "id": "d56d6ebf"
      },
      "source": [
        "## **6. Create a BioImage Model Zoo model**\n",
        "\n",
        "---\n",
        "\n",
        "Let's recreate a model based on parts of the loaded model description from above!\n",
        "\n",
        "`bioimageio.core` also implements functionality to create a model package compatible with the [BioImnageIO Model Spec](https://bioimage.io/docs/#/bioimageio_model_spec) ready to be shared via the [Bioimage Model Zoo](https://bioimage.io/#/).\n",
        "Here, we will use this functionality to create a new model with the finetuned weights.\n",
        "\n",
        "For this we are using some information from the previouse model.\n",
        "\n",
        "Run the following cell to export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed55e3b",
      "metadata": {
        "cellView": "form",
        "id": "9ed55e3b",
        "outputId": "a55944b1-6e1f-49a7-f173-ceaec3f1419e"
      },
      "outputs": [],
      "source": [
        "# ------\n",
        "# Information about the model\n",
        "\n",
        "#@markdown ##Export the new model to the bioimage model zoo format\n",
        "\n",
        "Trained_model_name = \"My new model\" #@param {type:\"string\"}\n",
        "Trained_model_description = \"Model finetuned\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Choose a test image\n",
        "Image_path = \"\"  #@param {type:\"string\"}\n",
        "training_data_bioimageio_id = \"zero/dataset_u-net_2d_multilabel_deepbacs\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Main directory where the new model checkpoint is saved\n",
        "Model_folder = \"\" #@param {type:\"string\"}\n",
        "MODEL_TORCHSCRIPT_PATH = os.path.join(Model_folder, \"finetuned_bioimageio_torchscript_model.pt\")\n",
        "output_path = os.path.join(Model_folder, \"finetuned_bioimageio_model.zip\")\n",
        "\n",
        "#####\n",
        "\n",
        "author_list = []\n",
        "for author in model.authors:\n",
        "  author_list.append(Author(name=author.name,\n",
        "                            affiliation=author.affiliation,\n",
        "                            github_user=author.github_user))\n",
        "\n",
        "Trained_model_license = model.license\n",
        "readme_path = model.documentation\n",
        "citation_list = []\n",
        "for citation in model.cite:\n",
        "  citation_list.append(CiteEntry(text=citation.text,\n",
        "                                 doi=citation.doi,\n",
        "                                 url=citation.url))\n",
        "\n",
        "\n",
        "## Define the new input\n",
        "\n",
        "# Load the input image, reshape it and save it as a numpy file\n",
        "new_input_path = f\"{Model_folder}/new_test_input.npy\"\n",
        "test_im = imread(Image_path)\n",
        "test_im = test_im[-256:, :256]\n",
        "test_im = np.expand_dims(test_im, axis=(0,1))\n",
        "np.save(new_input_path, np.float32(test_im))\n",
        "\n",
        "# Define the input axes (take the ones from previous model)\n",
        "input_axes = []\n",
        "for axis in model.inputs[0].axes:\n",
        "  if axis.id == \"batch\":\n",
        "    input_axes.append(BatchAxis(id=axis.id,\n",
        "                                 description=axis.description,\n",
        "                                 type=axis.type,\n",
        "                                 size=axis.size))\n",
        "  elif axis.id == \"channel\":\n",
        "    input_axes.append(ChannelAxis(id=axis.id,\n",
        "                                 description=axis.description,\n",
        "                                 type=axis.type,\n",
        "                                 channel_names=[Identifier(\"raw\")]))\n",
        "  else: # x, y or z\n",
        "    input_axes.append(SpaceInputAxis(id=axis.id,\n",
        "                                      description=axis.description,\n",
        "                                      type=axis.type,\n",
        "                                      unit=axis.unit,\n",
        "                                      scale=axis.scale,\n",
        "                                      concatenable=axis.concatenable,\n",
        "                                      size=axis.size))\n",
        "\n",
        "# Define the data description (which is the data type)\n",
        "data_descr = IntervalOrRatioDataDescr(type=\"float32\")\n",
        "\n",
        "# Define the preprocessing functions, we take the ones that were already defined\n",
        "preprocessing_list = model.inputs[0].preprocessing\n",
        "\n",
        "# Create the input tensor description\n",
        "new_input_descr = InputTensorDescr(id=TensorId(\"raw\"),\n",
        "                               axes=input_axes,\n",
        "                               test_tensor=FileDescr(source=new_input_path),\n",
        "                               data=data_descr,\n",
        "                               preprocessing=preprocessing_list\n",
        ")\n",
        "\n",
        "## Define the new output, in this step we don't have the output yet, so we are\n",
        "## creating a 'fake/auxiliar' output (which is created from the input) and then once\n",
        "## the model is created we will be able to create the prediction and create the real\n",
        "## output with it.\n",
        "\n",
        "# Create a temporal output\n",
        "aux_new_output_path = f\"{Model_folder}/aux_new_test_output.npy\"\n",
        "# The output is expected to have 2 channels, then concatenate two input images on channel axis\n",
        "np.save(aux_new_output_path, np.float32(np.concatenate((test_im,test_im), axis=1)))\n",
        "\n",
        "# Define the output axes (take the ones from previous model)\n",
        "output_axes = []\n",
        "for axis in model.outputs[0].axes:\n",
        "  if axis.id == \"batch\":\n",
        "    output_axes.append(BatchAxis(id=axis.id,\n",
        "                                 description=axis.description,\n",
        "                                 type=axis.type,\n",
        "                                 size=axis.size))\n",
        "  elif axis.id == \"channel\":\n",
        "    output_axes.append(ChannelAxis(id=axis.id,\n",
        "                                 description=axis.description,\n",
        "                                 type=axis.type,\n",
        "                                 channel_names=axis.channel_names))\n",
        "  else: # x, y or z\n",
        "    output_axes.append(SpaceOutputAxis(id=axis.id,\n",
        "                                      description=axis.description,\n",
        "                                      type=axis.type,\n",
        "                                      unit=axis.unit,\n",
        "                                      scale=axis.scale,\n",
        "                                      size=SizeReference(tensor_id=TensorId(\"raw\"),\n",
        "                                                         axis_id=axis.size.axis_id,\n",
        "                                                         offset=axis.size.offset)))\n",
        "\n",
        "# Define the posprocessing functions, we take the ones that were already defined\n",
        "# postprocessing_list = model.outputs[0].postprocessing\n",
        "postprocessing_list = [] # This model does not require postprocessing\n",
        "\n",
        "# Create the output tensor description\n",
        "new_output_descr = OutputTensorDescr(id=TensorId(\"prob\"),\n",
        "                                    axes=output_axes,\n",
        "                                    test_tensor=FileDescr(source=aux_new_output_path),\n",
        "                                    postprocessing=postprocessing_list)\n",
        "\n",
        "# Define the training data\n",
        "training_data = LinkedDataset(id=training_data_bioimageio_id)\n",
        "\n",
        "# Define the PyTorch architecture with the one you previously loaded\n",
        "# model_source = get_architecture_source(\"affable-shark\")\n",
        "pytorch_architecture = ArchitectureFromFileDescr(\n",
        "        source=download(model.weights.pytorch_state_dict.architecture.source,\n",
        "                        sha256=model.weights.pytorch_state_dict.architecture.sha256).path,\n",
        "        sha256=model.weights.pytorch_state_dict.architecture.sha256,\n",
        "        callable=model.weights.pytorch_state_dict.architecture.callable,\n",
        "        kwargs=model.weights.pytorch_state_dict.architecture.kwargs\n",
        "    )\n",
        "\n",
        "# Get PyTorch version\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    pytorch_version = Version(\"1.15\")\n",
        "else:\n",
        "    pytorch_version = Version(torch.__version__)\n",
        "\n",
        "# Define the weights using provided info\n",
        "weights = WeightsDescr(\n",
        "            torchscript=TorchscriptWeightsDescr(\n",
        "                source=MODEL_TORCHSCRIPT_PATH,\n",
        "                sha256=None,\n",
        "                pytorch_version=torch.__version__\n",
        "            )\n",
        "          )\n",
        "\n",
        "# We create the model, process the input image and create the model again with the correct output.\n",
        "for i in range(2):\n",
        "  # The test input and output data are passed as list because we support multiple inputs / outputs per model\n",
        "  my_model_descr = ModelDescr(\n",
        "      name = Trained_model_name,\n",
        "      description = Trained_model_description,\n",
        "      authors = author_list,\n",
        "      license = Trained_model_license,\n",
        "      documentation = readme_path,\n",
        "      weights= weights,\n",
        "      inputs = [new_input_descr],\n",
        "      outputs =  [new_output_descr],\n",
        "      tags=[\"in-silico-labeling\",\"pytorch\", \"cyclegan\", \"conditional-gan\",\n",
        "            \"zerocostdl4mic\", \"deepimagej\", \"actin\", \"dapi\", \"cells\", \"nuclei\",\n",
        "            \"fluorescence-light-microscopy\", \"2d\"],  # the tags are used to make models more findable on the website\n",
        "      cite = citation_list,\n",
        "      training_data = training_data,\n",
        "      # add_deepimagej_config=True,\n",
        "      )\n",
        "\n",
        "  if i == 0:\n",
        "\n",
        "    # Define the new input sample (taken from the new model description)\n",
        "    new_input_paths = {ipt.id: download(ipt.test_tensor).path for ipt in my_model_descr.inputs}\n",
        "\n",
        "    # The prediction pipeline expects a Sample object from bioimageio.core\n",
        "    input_sample = create_sample_for_model(\n",
        "        model=my_model_descr, inputs=new_input_paths, sample_id=\"my_demo_sample\"\n",
        "    )\n",
        "\n",
        "    # Create the new prediction\n",
        "    prediction = predict(model=my_model_descr, inputs=input_sample)\n",
        "\n",
        "    # Save the new prediction on a NumPy file\n",
        "    new_output_path = f\"{Model_folder}/new_test_output.npy\"\n",
        "    prediction_tensor = prediction.members[\"prob\"].data\n",
        "    np.save(os.path.join(new_output_path), prediction_tensor)\n",
        "\n",
        "    # Define the posprocessing functions, we take the ones that were already defined\n",
        "    # postprocessing_list = model.outputs[0].postprocessing\n",
        "    postprocessing_list = [] # This model does not require postprocessing\n",
        "\n",
        "    # Create the output tensor description\n",
        "    new_output_descr = OutputTensorDescr(id=TensorId(\"prob\"),\n",
        "                          axes=output_axes,\n",
        "                          test_tensor=FileDescr(source=new_output_path),\n",
        "                          postprocessing=postprocessing_list)\n",
        "\n",
        "# Check that the model works and display the result of the test\n",
        "summary = test_model(my_model_descr)\n",
        "summary.display()\n",
        "\n",
        "\n",
        "if summary.status == \"passed\":\n",
        "  # In case it has passed the test, save the bioimage.io model with the correct format\n",
        "  save_bioimageio_package(my_model_descr, output_path=Path(output_path))\n",
        "  print(\"The bioimage.io model was successfully exported to\", output_path)\n",
        "else:\n",
        "  print(\"The bioimage.io model was exported to\", output_path)\n",
        "  print(\"Some tests of the model did not work!l.\")\n",
        "  print(\"You can still download and test the model, but it may not work as expected.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
